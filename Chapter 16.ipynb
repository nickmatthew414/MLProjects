{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "n_steps=5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(size=n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print('_' * 20, 'Batch', index, '\\nX_batch')\n",
    "    print(X_batch.numpy())\n",
    "    print('=' * 5, '\\nY)batch')\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = tf.keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shakespeare_text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(['sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts([[8, 5, 15, 23, 12, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count\n",
    "print(max_id)\n",
    "print(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(units=128, return_sequences=True, input_shape=[None, max_id], \n",
    "                        dropout=.2, recurrent_dropout=.2),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, dropout=.2, recurrent_dropout=.2),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=max_id, activation=tf.nn.softmax))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "history = model.fit(dataset, steps_per_epoch=train_size//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import tensorflow as tf\n",
    "\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 1, 1).toordinal()\n",
    "    \n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "    \n",
    "    x = [MONTHS[dt.month - 1] + ' ' + dt.strftime('%d, %Y') for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print('{:25s}{:25s}'.format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADFJMNOSabceghilmnoprstuvy0123456789, '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS)))) + \"0123456789, \"\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X+1).to_tensor() # Using 0 as the padding token ID\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, y_train = create_dataset(10000)\n",
    "X_valid, y_valid = create_dataset(2000)\n",
    "X_test, y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.8177 - accuracy: 0.3482 - val_loss: 1.4038 - val_accuracy: 0.4696\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 1.3489 - accuracy: 0.5124 - val_loss: 1.2919 - val_accuracy: 0.5418\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 1.0756 - accuracy: 0.6150 - val_loss: 0.9297 - val_accuracy: 0.6532\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.8656 - accuracy: 0.6827 - val_loss: 0.7471 - val_accuracy: 0.7216\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.6145 - accuracy: 0.7685 - val_loss: 0.5148 - val_accuracy: 0.8015\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.5421 - accuracy: 0.8042 - val_loss: 0.5393 - val_accuracy: 0.8061\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.3204 - accuracy: 0.8885 - val_loss: 0.2371 - val_accuracy: 0.9230\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.4606 - accuracy: 0.8622 - val_loss: 0.5687 - val_accuracy: 0.8037\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.2923 - accuracy: 0.9204 - val_loss: 0.1581 - val_accuracy: 0.9657\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1078 - accuracy: 0.9812 - val_loss: 0.0818 - val_accuracy: 0.9851\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0596 - accuracy: 0.9922 - val_loss: 0.0508 - val_accuracy: 0.9932\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0363 - accuracy: 0.9966 - val_loss: 0.0318 - val_accuracy: 0.9967\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0230 - accuracy: 0.9985 - val_loss: 0.0211 - val_accuracy: 0.9987\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.0152 - accuracy: 0.9993 - val_loss: 0.0144 - val_accuracy: 0.9993\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0103 - accuracy: 0.9998 - val_loss: 0.0100 - val_accuracy: 0.9998\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0072 - accuracy: 0.9999 - val_loss: 0.0073 - val_accuracy: 0.9998\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9998\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9998\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 0.9999\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x137827d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(INPUT_CHARS)+1,\n",
    "                              output_dim=embedding_size,\n",
    "                              input_shape=[None]),\n",
    "    tf.keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(units=len(OUTPUT_CHARS)+1, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model = tf.keras.models.Sequential([encoder, \n",
    "                                   tf.keras.layers.RepeatVector(max_output_length),\n",
    "                                   decoder])\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
